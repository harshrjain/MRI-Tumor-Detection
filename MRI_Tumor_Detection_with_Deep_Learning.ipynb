{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshrjain/MRI-Tumor-Detection/blob/main/MRI_Tumor_Detection_with_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MRI Scan Detection - CNN**"
      ],
      "metadata": {
        "id": "3PdAizuwIMjK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "rT8dDKRyIZwG",
        "outputId": "17d440b4-5902-4d06-a12b-4f2365d0adbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from skimage import io\n",
        "from tensorflow.python.keras import Sequential\n",
        "from tensorflow.keras import layers, optimizers\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.initializers import glorot_uniform\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from IPython.display import display\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "KbBW0B0iIRIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_df = pd.read_csv('/content/data_mask.csv')\n",
        "brain_df.head()"
      ],
      "metadata": {
        "id": "JPy1XKUUJB5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_df.info()"
      ],
      "metadata": {
        "id": "F5Um64dlJXBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_df['mask'].value_counts().index"
      ],
      "metadata": {
        "id": "apYdGCR9JZ02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plotly to plot interactive bar chart\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure([go.Bar(x = brain_df['mask'].value_counts().index, y = brain_df['mask'].value_counts())])\n",
        "fig.update_traces(marker_color = 'rgb(0,200,0)', marker_line_color = 'rgb(0,255,0)',\n",
        "                  marker_line_width = 7, opacity = 0.6)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "IhAnuV6TJd6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_df.mask_path"
      ],
      "metadata": {
        "id": "-zs7ADQGJfrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mask_path = brain_df.mask_path[623]\n",
        "# print(mask_path)\n",
        "# plt.imshow(cv2.imread('/content/drive/MyDrive/MRI Datasets/TCGA_HT_7694_19950404/TCGA_HT_7694_19950404_6_mask.tif'))\n",
        "\n",
        "mask_path = brain_df.mask_path[623]\n",
        "print(mask_path)\n",
        "\n",
        "full_path = f'/content/drive/MyDrive/MRI Datasets/{mask_path}'\n",
        "plt.imshow(cv2.imread(full_path))\n",
        "plt.show() # Important: Add plt.show() to display the image"
      ],
      "metadata": {
        "id": "Qz6v9ifJwaqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(cv2.imread(f'/content/drive/MyDrive/MRI Datasets/{mask_path}'))"
      ],
      "metadata": {
        "id": "UKHMaMBvnm0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imread(f'/content/drive/MyDrive/MRI Datasets/{mask_path}').max()"
      ],
      "metadata": {
        "id": "iUpkny0EnqtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv2.imread(f'/content/drive/MyDrive/MRI Datasets/{mask_path}').min()"
      ],
      "metadata": {
        "id": "e5TpLxZYns3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic visualizations: Visualize the images (MRI and Mask) in the dataset separately\n",
        "import random\n",
        "fig, axs = plt.subplots(6,2, figsize=(16,32))\n",
        "count = 0\n",
        "for x in range(6):\n",
        "  i = random.randint(0, len(brain_df)) # select a random index\n",
        "  axs[count][0].title.set_text(\"Brain MRI\") # set title\n",
        "  temp1 = brain_df.image_path[i]\n",
        "  axs[count][0].imshow(cv2.imread(f'/content/drive/MyDrive/MRI Datasets/{temp1}')) # show MRI\n",
        "  axs[count][1].title.set_text(\"Mask - \" + str(brain_df['mask'][i])) # plot title on the mask (0 or 1)\n",
        "  temp2 = brain_df.mask_path[i]\n",
        "  axs[count][1].imshow(cv2.imread(f'/content/drive/MyDrive/MRI Datasets/{temp2}')) # Show corresponding mask\n",
        "  count += 1\n",
        "\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "BhPkXzB8qtp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "fig, axs = plt.subplots(12, 3, figsize = (20, 50))\n",
        "for i in range(len(brain_df)):\n",
        "  if brain_df['mask'][i] ==1 and count <12:\n",
        "    temp1 = brain_df.image_path[i]\n",
        "    img = io.imread(f'/content/drive/MyDrive/MRI Datasets/{temp1}')\n",
        "    axs[count][0].title.set_text('Brain MRI')\n",
        "    axs[count][0].imshow(img)\n",
        "\n",
        "    temp2 = brain_df.mask_path[i]\n",
        "    mask = io.imread(f'/content/drive/MyDrive/MRI Datasets/{temp2}')\n",
        "    axs[count][1].title.set_text('Mask')\n",
        "    axs[count][1].imshow(mask, cmap = 'gray')\n",
        "\n",
        "\n",
        "    img[mask == 255] = (255, 0, 0)\n",
        "    axs[count][2].title.set_text('MRI with Mask')\n",
        "    axs[count][2].imshow(img)\n",
        "    count+=1\n",
        "\n",
        "fig.tight_layout()\n"
      ],
      "metadata": {
        "id": "8fN2fNepr6Nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the patient id column\n",
        "brain_df_train = brain_df.drop(columns = ['patient_id'])\n",
        "brain_df_train.shape"
      ],
      "metadata": {
        "id": "gOQ25HKzsegl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the data in mask column to string format, to use categorical mode in flow_from_dataframe\n",
        "# You will get this error message if you comment out the following code line:\n",
        "# TypeError: If class_mode=\"categorical\", y_col=\"mask\" column values must be type string, list or tuple.\n",
        "\n",
        "brain_df_train['mask'] = brain_df_train['mask'].apply(lambda x: str(x))\n",
        "brain_df_train.info()"
      ],
      "metadata": {
        "id": "QTL6OX0Usg96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(brain_df_train, test_size = 0.15)"
      ],
      "metadata": {
        "id": "NcLvWSgCskSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras_preprocessing\n"
      ],
      "metadata": {
        "id": "bTYzpCrTsmn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # create a image generator\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# # Create a data generator which scales the data from 0 to 1 and makes validation split of 0.15\n",
        "datagen = ImageDataGenerator(rescale=1./255., validation_split = 0.15)"
      ],
      "metadata": {
        "id": "TMPTzrogszU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator=datagen.flow_from_dataframe(\n",
        "dataframe=train,\n",
        "directory= '/content/drive/MyDrive/MRI Datasets/',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "subset=\"training\",\n",
        "batch_size=16,\n",
        "shuffle=True,\n",
        "class_mode=\"categorical\",\n",
        "target_size=(256,256))\n",
        "\n",
        "\n",
        "valid_generator=datagen.flow_from_dataframe(\n",
        "dataframe=train,\n",
        "directory= '/content/drive/MyDrive/MRI Datasets/',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "subset=\"validation\",\n",
        "batch_size=16,\n",
        "shuffle=True,\n",
        "class_mode=\"categorical\",\n",
        "target_size=(256,256))\n",
        "\n",
        "# Create a data generator for test images\n",
        "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "dataframe=test,\n",
        "directory= '/content/drive/MyDrive/MRI Datasets/',\n",
        "x_col='image_path',\n",
        "y_col='mask',\n",
        "batch_size=16,\n",
        "shuffle=False,\n",
        "class_mode='categorical',\n",
        "target_size=(256,256))\n",
        "\n"
      ],
      "metadata": {
        "id": "m37vN0LXtAlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ResNet50 base model\n",
        "basemodel = ResNet50(weights = 'imagenet', include_top = False, input_tensor = Input(shape=(256, 256, 3)))"
      ],
      "metadata": {
        "id": "s2XkXvQ6tr8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basemodel.summary()"
      ],
      "metadata": {
        "id": "vEt9zf8atyId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeze the model weights\n",
        "\n",
        "for layer in basemodel.layers:\n",
        "  layers.trainable = False"
      ],
      "metadata": {
        "id": "MU6wuFXUt43L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add classification head to the base model\n",
        "\n",
        "headmodel = basemodel.output\n",
        "headmodel = AveragePooling2D(pool_size = (4,4))(headmodel)\n",
        "headmodel = Flatten(name= 'flatten')(headmodel)\n",
        "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "headmodel = Dropout(0.3)(headmodel)#\n",
        "headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "headmodel = Dropout(0.3)(headmodel)\n",
        "#headmodel = Dense(256, activation = \"relu\")(headmodel)\n",
        "#headmodel = Dropout(0.3)(headmodel)\n",
        "headmodel = Dense(2, activation = 'softmax')(headmodel)\n",
        "\n",
        "model = Model(inputs = basemodel.input, outputs = headmodel)"
      ],
      "metadata": {
        "id": "gncaNVCxt7v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bOvQL90et-WV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])"
      ],
      "metadata": {
        "id": "piliZMiJuApe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, steps_per_epoch= train_generator.n // 16, epochs = 1, validation_data= valid_generator, validation_steps= valid_generator.n // 16, callbacks=[checkpointer, earlystopping])"
      ],
      "metadata": {
        "id": "2V4sISS-xLew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"classifier-resnet-model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "metadata": {
        "id": "3UagVC8T9bJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load pretrained model (instead of training the model for 1+ hours)\n",
        "# with open('resnet-50-MRI.json', 'r') as json_file:\n",
        "#     json_savedModel= json_file.read()\n",
        "# # load the model\n",
        "# model = tf.keras.models.model_from_json(json_savedModel)\n",
        "# model.load_weights('weights.hdf5')\n",
        "# model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics= [\"accuracy\"])\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# ... your model definition ... (e.g., model = ResNet50(...))\n",
        "\n",
        "# Save model architecture to JSON\n",
        "model_json = model.to_json()\n",
        "with open(\"resnet-50-MRI.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "# Save model weights to HDF5\n",
        "#model.save_weights(\"weights.hdf5\")  # Or\n",
        "model.save(\"model_name.keras\") #for Keras format"
      ],
      "metadata": {
        "id": "xGHJQUBr9gLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predict = model.predict(test_generator, steps = test_generator.n // 16, verbose =1)"
      ],
      "metadata": {
        "id": "AYjRqkdqACv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predict"
      ],
      "metadata": {
        "id": "9ndoU7wiAQc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict = []\n",
        "\n",
        "for i in test_predict:\n",
        "  predict.append(str(np.argmax(i)))\n",
        "\n",
        "predict = np.asarray(predict)"
      ],
      "metadata": {
        "id": "5EqL293tBWok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict"
      ],
      "metadata": {
        "id": "-rFH5fIwBYP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original = np.asarray(test['mask'])[:len(predict)]\n",
        "len(original)"
      ],
      "metadata": {
        "id": "gFy-EtSbBZmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the accuracy of the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(original, predict)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "Qt47zVK1BcVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(original, predict)\n",
        "plt.figure(figsize = (7,7))\n",
        "sns.heatmap(cm, annot=True)"
      ],
      "metadata": {
        "id": "QErpn4GCBeXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(original, predict, labels = [0,1])\n",
        "print(report)"
      ],
      "metadata": {
        "id": "PKgaTrnyBhcA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dataframe containing MRIs which have masks associated with them.\n",
        "brain_df_mask = brain_df[brain_df['mask'] == 1]\n",
        "brain_df_mask.shape"
      ],
      "metadata": {
        "id": "HoCRThCZBkgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into train and test data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val = train_test_split(brain_df_mask, test_size=0.15)\n",
        "X_test, X_val = train_test_split(X_val, test_size=0.5)"
      ],
      "metadata": {
        "id": "abtndRWEBmUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ids = list(X_train.image_path)\n",
        "train_mask = list(X_train.mask_path)\n",
        "\n",
        "val_ids = list(X_val.image_path)\n",
        "val_mask= list(X_val.mask_path)"
      ],
      "metadata": {
        "id": "RHJOCR50BnfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utilities file contains the code for custom loss function and custom data generator\n",
        "from utilities import DataGenerator\n",
        "\n",
        "# create image generators\n",
        "\n",
        "training_generator = DataGenerator(train_ids,train_mask)\n",
        "validation_generator = DataGenerator(val_ids,val_mask)"
      ],
      "metadata": {
        "id": "mqdFdqCLBoh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resblock(X, f):\n",
        "\n",
        "\n",
        "  # make a copy of input\n",
        "  X_copy = X\n",
        "\n",
        "  # main path\n",
        "  # Read more about he_normal: https://medium.com/@prateekvishnu/xavier-and-he-normal-he-et-al-initialization-8e3d7a087528\n",
        "\n",
        "  X = Conv2D(f, kernel_size = (1,1) ,strides = (1,1),kernel_initializer ='he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  X = Conv2D(f, kernel_size = (3,3), strides =(1,1), padding = 'same', kernel_initializer ='he_normal')(X)\n",
        "  X = BatchNormalization()(X)\n",
        "\n",
        "  # Short path\n",
        "  # Read more here: https://towardsdatascience.com/understanding-and-coding-a-resnet-in-keras-446d7ff84d33\n",
        "\n",
        "  X_copy = Conv2D(f, kernel_size = (1,1), strides =(1,1), kernel_initializer ='he_normal')(X_copy)\n",
        "  X_copy = BatchNormalization()(X_copy)\n",
        "\n",
        "  # Adding the output from main path and short path together\n",
        "\n",
        "  X = Add()([X,X_copy])\n",
        "  X = Activation('relu')(X)\n",
        "\n",
        "  return X"
      ],
      "metadata": {
        "id": "dYYi7HIYBqPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to upscale and concatenate the values passsed\n",
        "def upsample_concat(x, skip):\n",
        "  x = UpSampling2D((2,2))(x)\n",
        "  merge = Concatenate()([x, skip])\n",
        "\n",
        "  return merge"
      ],
      "metadata": {
        "id": "yX1ftljEBs5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (256,256,3)\n",
        "\n",
        "# Input tensor shape\n",
        "X_input = Input(input_shape)\n",
        "\n",
        "# Stage 1\n",
        "conv1_in = Conv2D(16,3,activation= 'relu', padding = 'same', kernel_initializer ='he_normal')(X_input)\n",
        "conv1_in = BatchNormalization()(conv1_in)\n",
        "conv1_in = Conv2D(16,3,activation= 'relu', padding = 'same', kernel_initializer ='he_normal')(conv1_in)\n",
        "conv1_in = BatchNormalization()(conv1_in)\n",
        "pool_1 = MaxPool2D(pool_size = (2,2))(conv1_in)\n",
        "\n",
        "# Stage 2\n",
        "conv2_in = resblock(pool_1, 32)\n",
        "pool_2 = MaxPool2D(pool_size = (2,2))(conv2_in)\n",
        "\n",
        "# Stage 3\n",
        "conv3_in = resblock(pool_2, 64)\n",
        "pool_3 = MaxPool2D(pool_size = (2,2))(conv3_in)\n",
        "\n",
        "# Stage 4\n",
        "conv4_in = resblock(pool_3, 128)\n",
        "pool_4 = MaxPool2D(pool_size = (2,2))(conv4_in)\n",
        "\n",
        "# Stage 5 (Bottle Neck)\n",
        "conv5_in = resblock(pool_4, 256)\n",
        "\n",
        "# Upscale stage 1\n",
        "up_1 = upsample_concat(conv5_in, conv4_in)\n",
        "up_1 = resblock(up_1, 128)\n",
        "\n",
        "# Upscale stage 2\n",
        "up_2 = upsample_concat(up_1, conv3_in)\n",
        "up_2 = resblock(up_2, 64)\n",
        "\n",
        "# Upscale stage 3\n",
        "up_3 = upsample_concat(up_2, conv2_in)\n",
        "up_3 = resblock(up_3, 32)\n",
        "\n",
        "# Upscale stage 4\n",
        "up_4 = upsample_concat(up_3, conv1_in)\n",
        "up_4 = resblock(up_4, 16)\n",
        "\n",
        "# Final Output\n",
        "output = Conv2D(1, (1,1), padding = \"same\", activation = \"sigmoid\")(up_4)\n",
        "\n",
        "model_seg = Model(inputs = X_input, outputs = output )\n"
      ],
      "metadata": {
        "id": "MIfN-C2ABud0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_seg.summary()"
      ],
      "metadata": {
        "id": "NIgYdTzmBwg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from utilities import focal_tversky, tversky_loss, tversky\n",
        "from utilities import focal_tversky, tversky_loss,tversky"
      ],
      "metadata": {
        "id": "J3hoQ6r-Byv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "legacy_adam = tf.keras.optimizers.Adam(learning_rate = 0.05, epsilon = 0.1)\n",
        "model_seg.compile(optimizer = legacy_adam, loss = focal_tversky, metrics = [tversky])"
      ],
      "metadata": {
        "id": "P2aQ4R98Bz33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # use early stopping to exit training if validation loss is not decreasing even after certain epochs (patience)\n",
        "# earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "\n",
        "# # save the best model with least validation loss\n",
        "# checkpointer = ModelCheckpoint(filepath=\"classifier-resnet-weights.keras\", verbose=1, save_best_only=True)"
      ],
      "metadata": {
        "id": "uW31yCKWuCt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model_seg.fit(training_generator, epochs = 1, validation_data = validation_generator, callbacks = [checkpointer, earlystopping])"
      ],
      "metadata": {
        "id": "HySrpuLcCoLO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}